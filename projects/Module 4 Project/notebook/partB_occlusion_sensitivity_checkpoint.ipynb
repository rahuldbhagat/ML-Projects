{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4415ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occlusion Sensitivity Analysis (Part-B)\n",
    "# Kernel: Python (ml_torch)\n",
    "# Loads best model automatically from checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1568f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaf7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_ROOT = \"../dataset\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OCCLUSION_SIZE = 8\n",
    "STRIDE = 2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = ImageFolder(os.path.join(DATASET_ROOT, \"test\"), transform=transform)\n",
    "class_names = test_dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4684246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, conv_filters, fc_layers, use_pool=True, stride=1, num_classes=None):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for f in conv_filters:\n",
    "            layers.append(nn.Conv2d(in_channels, f, 3, stride=stride, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            if use_pool:\n",
    "                layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = f\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 84, 84)\n",
    "            out = self.features(dummy)\n",
    "            flat_dim = out.view(1, -1).size(1)\n",
    "\n",
    "        fc = []\n",
    "        in_dim = flat_dim\n",
    "        for h in fc_layers:\n",
    "            fc.append(nn.Linear(in_dim, h))\n",
    "            fc.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "        fc.append(nn.Linear(in_dim, num_classes))\n",
    "        self.classifier = nn.Sequential(*fc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdccf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(\"best_model_checkpoint.pth\", map_location=DEVICE)\n",
    "cfg = checkpoint[\"config\"]\n",
    "\n",
    "model = ConfigurableCNN(\n",
    "    conv_filters=cfg[\"conv_filters\"],\n",
    "    fc_layers=cfg[\"fc_layers\"],\n",
    "    use_pool=cfg[\"use_pool\"],\n",
    "    stride=cfg[\"stride\"],\n",
    "    num_classes=len(class_names)\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model config:\", cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few misclassified test images\n",
    "\n",
    "misclassified = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img, label in test_dataset:\n",
    "        img = img.unsqueeze(0).to(DEVICE)\n",
    "        output = model(img)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "\n",
    "        if pred != label:\n",
    "            misclassified.append((img.cpu().squeeze(), label, pred))\n",
    "        \n",
    "        if len(misclassified) == 5:\n",
    "            break\n",
    "\n",
    "# Plot misclassified images\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, (img, true_label, pred_label) in enumerate(misclassified):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    plt.title(f\"T: {class_names[true_label]}\\nP: {class_names[pred_label]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def occlusion_sensitivity(model, image, true_label):\n",
    "    _, H, W = image.shape\n",
    "    confidence = np.zeros((H // STRIDE, W // STRIDE))\n",
    "    image = image.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, H, STRIDE):\n",
    "            for j in range(0, W, STRIDE):\n",
    "                occluded = image.clone()\n",
    "                occluded[:, i:i+OCCLUSION_SIZE, j:j+OCCLUSION_SIZE] = 0.0\n",
    "                out = model(occluded.unsqueeze(0))\n",
    "                prob = torch.softmax(out, 1)[0, true_label].item()\n",
    "                confidence[i//STRIDE, j//STRIDE] = prob\n",
    "    return confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = list(range(10))\n",
    "\n",
    "for idx in indices:\n",
    "    img, label = test_dataset[idx]\n",
    "    conf_map = occlusion_sensitivity(model, img, label)\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(conf_map, cmap=\"hot\")\n",
    "    plt.title(\"Occlusion Sensitivity\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_torch)",
   "language": "python",
   "name": "ml_torch"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
